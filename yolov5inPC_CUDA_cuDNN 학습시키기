yolov5 in PC
사양:  NVIDIA GeForce RTX 4060 Laptop GPU 16G RAM

1. CUDA cuDNN 설치 합니다.
Compute Unified Device Architecture
CUDA Deep Neural Network library

CUDA 버전확인: nvidia-smi
CUDA : 12.0
CUDA capability : 8.6
https://developer.nvidia.com/cuda-toolkit-archive
https://developer.nvidia.com/rdp/cudnn-archive
CUDA Toolkit 12.0.0 (December 2022), Versioned Online Documentation, windows11
C:\Users\nayou\AppData\Local\Temp\cuda
nvidia 가입 후 로그인
Download cuDNN v8.9.7 (December 5th, 2023), for CUDA 12.x
Local Installers for Windows and Linux, Ubuntu(x86_64, armsbsa)
Local Installer for Windows (Zip)
-SDK설치경로에 다운로드받은 폴더를 추가합니다.(include, lib, bin)
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.0

2. git을 다운로드받습니다.
https://git-scm.com/
C:\Program Files\Git

3. git환경변수를 추가해주고 재부팅합니다.
C:\Program Files\Git\bin
명령프롬프트에서 KMP_DUPLICATE_LIB_OK=TRUE 환경변수를 등록해줍니다.

1. anaconda를 설치합니다. vscode를 설치합니다.
https://code.visualstudio.com/download
C:\Users\nayou\AppData\Local\Programs\Microsoft VS Code

2. github에서 yolov5를 다운로드합니다.
https://github.com/ultralytics/yolov5
code > download zip
c:\yolov5

3. anaconda prompt를 관리자 권한으로 실행하고 아래대로 실행합니다.
conda create --name py39 python=3.9
conda activate py39
conda update conda
cd C:\yolov5
pip install -r requirements.txt

4. Run yolov5 in Local PC with CLI
python detect.py --weights yolov5n.pt --img 640 --conf 0.25 --source data/images
runs/detect/exp

python c:/yolov5/detect.py --weights C:\yolov5result\exp20\weights\best.pt --img 640 --conf 0.50 --source C:\images --device 0

5. Train(opendataset COCO128, 학습 오픈데이터셋으로 yolov5학습 튜토리얼)
python train.py --img 640 --batch 1 --epochs 1 --data coco128.yaml --weights yolov5n.pt
Results saved to runs\train\exp2

6. 직접만든 데이터셋으로 학습시키기
python c:/yolov5/train.py --img 640 --batch 16 --epochs 250 --data c:/data.yaml --weights c:/yolov5/models/yolov5m.pt --device 0
(base) c:\yolov5>python c:/yolov5/train.py --img 640 --batch 1 --epochs 100 --data c:/cup_test/data.yaml --weights yolov5n.pt --device 0
 Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size
       0/99      0.51G    0.05364     0.0261    0.01911          2        640: 100%|██████████| 643/643 [00:35<00:
                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 35/35 [
                   all         69         78       0.79       0.16      0.148     0.0492

Model summary: 157 layers, 1763224 parameters, 0 gradients, 4.1 GFLOPs
                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 35/35 [
                   all         69         78      0.921      0.986      0.992      0.836
                bottle         69          3      0.827          1      0.995      0.841
                   cup         69         60          1      0.972      0.995      0.848
               tumbler         69         15      0.937      0.986      0.987      0.819
Results saved to runs\train\exp12

7. 만들어진 best.pt로 검출하기
python c:/yolov5/detect.py --weights C:\yolov5\runs\train\exp25\weights\best.pt --img 640 --conf 0.70 --source C:\images --device 0


8. 최적화
python c:/yolov5/train.py --img 640 --batch 16 --epochs 250 --data c:/data.yaml --weights c:/yolov5/models/yolov5m.pt --device 0 --name experiment_name --optimizer Adam --project project_directory
Adam은 Adaptive Moment Estimation(적응적 모멘트 추정)의 약자로, SGD의 단점을 보완하기 위해 제안된 옵티마이저입니다.
각 파라미터마다 학습률을 조정하고, 지수 가중 이동 평균을 사용하여 각 매개변수의 스케일을 조정합니다.
이러한 특징으로 인해 Adam은 일반적으로 SGD보다 더 빠르게 수렴하고 더 나은 성능을 보입니다.
-bach를 바꿔 학습시킵니다
python c:/yolov5/train.py --img 640 --batch 8 --epochs 100 --data c:/data.yaml --weights c:/yolov5/models/yolov5m.pt --device 0 --name exp --optimizer Adam --project c:\yolov5result
python c:/yolov5/train.py --img 640 --batch 16 --epochs 50 --data c:/data.yaml --weights c:/yolov5/models/yolov5m.pt --device 0 --name exp --optimizer SGD --project c:/yolov5result

옵션설명
python : 스크립트를 실행하는 데 사용할 Python 인터프리터 지정
c:/yolov5/train.py : YOLOv5를 훈련하는 데 사용되는 Python 스크립트 train.py의 경로
--img 640 : 이미지의 크기를 640X640 픽셀로 설정, 이미지의 크기가 클 수록 성능이 향상되지만 훈련시간이 늘어날 수 있음
--batch 8 : 배치의 크기설정, 한 번에 모델에 공급되는 이미지의 수, 한번에 8개의 이미지가 모델에 공급, 
            더 큰 배치를 사용하면 GPU가 더 많은 데이터를 처리할 수 있어 훈련 속도가 향상되지만, 그만큼  더 많은 메모리가 필요함
--epochs 100 : YOLOv5훈련 중에 전체 데이터셋을 몇 번 반복해서 사용할지를 결정함, 모델이 데이터셋을 한번 통과할 때마다 한 번의 에포크가 진행됨
               에포크 수는 훈련과정에서 데이터셋을 모델에 노출시키는 횟수 결정, 일반적으로 더 많은 에포크를 사용할수록 모델의 성능이 향상,
               과적(overfitting)을 방지하기 위해 적절한 수준에서 훈련을 중단해야함
--data c:/data.yaml : 데이터 구성을 정의 하는 YAML파일의 경로, 데이터 구성은 클래스 이름, 학습 및 검증 이미지의 경로 등 포함
--weights c:/yolov5/models/yolov5m.pt : 사전 훈련된 모델 가중치 파일의 경로, 훈련을 시작하기 전에 초기 가중치로 사용
--device 0 : 사용할 디바이스 설정, 0은 첫번째 GPU를 가리킴, GPU가 없는 경우 --device cpu로 설정
--name exp : 훈련 실험의 이름 설정, 훈련 중에 생성된 결과 디렉토리에 사용
--optimizer Adam : 옵티마이저 설정
--optimizer SDG : Stochastic Gradient Descent는 딥러닝에서 가장 기본적인 최적화 알고리즘
--project c:/yolov5result : 결과를 저장할 프로젝트 디렉토리의 경로, 훈련 중에 생성된 모든 파일 및 로그가 저장됨

data.yaml
train: c:/cup_test/train/images #휸련 데이터셋 이미지의 디렉토리 경로 지정
val: c:/cup_test/valid/images #검증 데이터셋 이미지의 디렉토리 경로 지정
test: c:/cup_test/test/images #테스트 데이터셋 이미지의 디렉토리 경로 지정

nc: 3 //데이터셋에 있는 클래스의 총개수 
names: ['bottle', 'cup', 'tumbler'] #클래스 이름의 리스트를 제공, nc의 숫자와 동일한 길이를 가지고 각 클래스이름은 따옴표속에 넣음

#데이터 증강(argumentation), YOLOv5에서 훈련할 때 데이터를 다양하게 변형하여 모델의 일반화 성능을 향상
mosaic: True  # Mosaic 데이터 증강 활성화, 네 개의 이미지를 하나의 이미지로 결합하여 훈련 데이터셋을 확장하는 방법,
              # 이를 통해 모델은 여러 객체와 배경을 동시에 볼 수 있으며, 모델의 일반화 성능을 향상시킬 수 있음
mixup: 0.5  # Mixup 데이터 증강 비율 (0 ~ 1), 데이터 증강을 적용하는 비율
            # 두 개의 이미지를 섞어서 새로운 이미지를 생성하는 방법, 
            # 두 이미지의 픽셀값을 선형으로 조합하여새로운 이미지를 생성
            # 모델은 다양한 이미지를 처리하고 과적합을 줄일 수있음.
            # 0에 가까울 수록 Mixup을 적게 적용하고, 1에 가까울 수록 많이 적용함

roboflow:
  workspace: nayoungkim-xrg93
  project: cup-krqrq
  version: 20
  license: CC BY 4.0
  url: https://universe.roboflow.com/nayoungkim-xrg93/cup-krqrq/dataset/20

개채 감지 모델의 훈련 모니터링(Yolov5, You Only Look Once)
      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size
       0/99      3.19G    0.05293    0.02123    0.01472          9        640: 100%|██████████| 83/83 [00:14<00:00,  5.
                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 6/6 [00:00<0
                   all         96         96          0          0          0          0

Epoch : 훈련 진행 상황
GPU_mem : 훈련 중 GPU메모리 사용량

!!세가지 손실을 최소화하고 낮은값으로 유지하는데 중점을 둠!!
box_loss : 박스손실, 바운딩 박스 위치 지정과 관련된 손실
obj_loss : 객체손실, 주어진 바운딩 박스에 개체가 있는지 예측하는 손실
cls_loss : 감지된 객체의 클래스를 예측하는 손실

Instances : 각 에포크마다 감지된 개체 인스턴스의 수, 모델이 객체를 감지하는 능력 파악
Size : 훈련 중 사용된 입력 이미지의 크기

감지매트릭
P : 정밀도, 모든 예측된 양성 감지 사이에서 실제 양성 감지의 비율
R : 재현율, 모든 실제 양성 인스턴스 사이에서 실제 양성 감지의 비율
mAP50 : IoU(Intersection over Union) 임계 값이 0.5인 경우의 평균 정밀도 객체 감지에 일반적으로 사용되는 지표
mAP50-95 : 0.5에서 0.95까지의 IoU임계 값 범위에서의 평균 정밀

(최적화...)
python c:/yolov5/train.py --img 640 --batch 16 --epochs 50 --data c:/data.yaml --weights c:/yolov5/models/yolov5m.pt --device 0 --name exp --optimizer SGD --project c:/yolov5result
python c:/yolov5/train.py --img 640 --batch 16 --epochs 50 --data c:/data.yaml --weights c:/yolov5/models/yolov5m.pt --device 0 --name exp --optimizer Adam --project c:/yolov5result
python c:/yolov5/detect.py --weights C:\yolov5result\exp20\weights\best.pt --img 640 --conf 0.50 --source C:\images --device 0

NVIDIA GeForce RTX 4060 Laptop GPU 16G RAM
11:04 ~ 11:31
python c:/yolov5/train.py --img 640 --batch 16 --epochs 100 --data c:/data.yaml --weights c:/yolov5/models/yolov5m.pt --device 0
python c:/yolov5/detect.py --weights C:\yolov5\runs\train\exp28\weights\best.pt --img 640 --conf 0.50 --source C:\images --device 0

--컵인식률90%이상나옴
python c:/yolov5/train.py --img 640 --batch 16 --epochs 200 --data c:/data.yaml --weights c:/yolov5/models/yolov5m.pt --device 0
python c:/yolov5/detect.py --weights C:\yolov5\runs\train\exp35\weights\best.pt --img 640 --conf 0.50 --source C:\images --device 0

--완성된bets.pt로 (C:\yolov5\runs\train\exp35\weights\best.pt) 웹캠 실시간 감지프로그램 작성 및 테스트(90%이상 인식률)
#camDetect.py
import cv2
import torch
from yolov5.models.experimental import attempt_load
from yolov5.utils.general import non_max_suppression
from pathlib import Path

# Initialize YOLOv5 model
weights = 'C:/yolov5/runs/train/exp35/weights/best.pt'
device = 'cpu'  # Assuming you want to run inference on CPU
model = torch.load(weights, map_location=device)['model'].float()  # Load model
model.eval()  # Set model to evaluation mode
stride = int(model.stride.max())

# Open webcam
cap = cv2.VideoCapture(0)

while cap.isOpened():
    # Read frame from webcam
    ret, frame = cap.read()
    if not ret:
        print('Error reading frame from webcam')
        break

    # Resize frame to match model's expected input size
    img = cv2.resize(frame, (640, 480))
    
    # Convert image to torch tensor
    img = torch.from_numpy(img).to(device)
    img = img.float()  # Convert to float
    img /= 255.0  # Normalize to 0-1
    
    # Ensure correct shape and number of channels
    img = img.permute(2, 0, 1).unsqueeze(0)  # Change shape to [1, 3, H, W]

    # Inference
    pred = model(img)[0]

    # Display results
    for det in pred:
        det[:, :4] *= torch.tensor([frame.shape[1]/640, frame.shape[0]/480, frame.shape[1]/640, frame.shape[0]/480], device=device)  # Scale coordinates to original frame size
        det[:, :4] = det[:, :4].clamp(min=0, max=frame.shape[1])  # Clamp coordinates within frame
        
        # Ensure det is a tensor before passing it to non_max_suppression
        if isinstance(det, torch.Tensor):
            det = non_max_suppression(det.unsqueeze(0), 0.4, 0.5)[0]  # Perform non-maximum suppression
            if det is not None and len(det):
                for *xyxy, conf, cls in det:
                    label = f'{model.names[int(cls)]} {conf:.2f}'
                    cv2.rectangle(frame, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (0, 255, 0), 2)
                    cv2.putText(frame, label, (int(xyxy[0]), int(xyxy[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    cv2.imshow('YOLOv5 Object Detection', frame)
    
    # Check for quit key
    if cv2.waitKey(1) == ord('q'):
        break

# Release resources
cap.release()
cv2.destroyAllWindows()

--best.pt로 pkl파일만들기(c++ & torch)
import os
import pickle

# 가중치 파일 경로
weights_path = "C:\\yolov5\\runs\\train\\exp35\\weights\\best.pt"
# 저장할 파일 경로 (예: 사용자의 홈 디렉토리 아래의 'Documents' 디렉토리)
output_path = os.path.join(os.path.expanduser("~"), "Documents", "constants.pkl")

# 가중치 파일을 읽어서 바이너리로 읽기
with open(weights_path, 'rb') as f:
    model_weights = f.read()

# constants.pkl 파일에 저장
with open(output_path, 'wb') as f:
    f.write(model_weights)

print(f"'constants.pkl' 파일이 '{output_path}' 경로에 생성되었습니다.")


--python으로 학습시켜 constants.pkl파일 만들기
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import yaml

# 데이터셋 설정 로드
with open('c:/data.yaml', 'r', encoding='utf-8') as f:
    data_config = yaml.safe_load(f)

# 데이터셋 클래스 정의
class CustomDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform

    def __len__(self):
        return len(self.root_dir)

    def __getitem__(self, idx):
        img_name = 'c://image.jpg'  # 이미지 파일명을 포함하여 경로 생성
        image = Image.open(img_name)
        label = 0  # 클래스 라벨은 임의로 0으로 설정
        if self.transform:
            image = self.transform(image)
        return image, label

# 모델 정의
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(28*28, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)  # 이진 분류를 위해 출력 뉴런 개수를 1로 변경

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.sigmoid(self.fc3(x))  # 이진 분류를 위해 시그모이드 함수 사용
        return x

# 데이터셋 변환 및 로드
transform = transforms.Compose([
    transforms.Grayscale(),  # 흑백 이미지로 변환
    transforms.Resize((28, 28)),  # 이미지 크기를 28x28로 조정
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

train_dataset = CustomDataset(data_config['train'], transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# 손실 함수 정의
criterion = nn.BCELoss()  # 이진 교차 엔트로피 손실 함수 사용

# 모델 초기화
model = SimpleNN()

# 최적화 알고리즘 선택
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 모델 훈련
num_epochs = 5
for epoch in range(num_epochs):
    running_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        outputs = outputs.squeeze(dim=1)  # 2D 텐서를 1D 텐서로 변환
        loss = criterion(outputs, labels.float())  # BCELoss를 사용하므로 레이블을 float으로 변환
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')

# 모델의 상태 저장
torch.save(model.state_dict(), 'constants.pkl')

[output]
Epoch [1/5], Loss: 0.6159849762916565
Epoch [2/5], Loss: 0.4989442527294159
Epoch [3/5], Loss: 0.40619611740112305
Epoch [4/5], Loss: 0.31730762124061584
Epoch [5/5], Loss: 0.2393183559179306

-- c++에서 모델로드가 계속 에러나서 
-- torch script로 만들어 모델로드 성공
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import yaml

# 데이터 설정 로드
with open('c:/data.yaml', 'r', encoding='utf-8') as f:
    data_config = yaml.safe_load(f)

# 데이터셋 클래스 정의
class CustomDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform

    def __len__(self):
        return len(self.root_dir)

    def __getitem__(self, idx):
        img_name = 'c://image.jpg'  # 이미지 파일명을 포함하여 경로 생성
        image = Image.open(img_name)
        label = 0  # 클래스 라벨은 임의로 0으로 설정
        if self.transform:
            image = self.transform(image)
        return image, label

# 모델 정의
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(28*28, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)  # 이진 분류를 위해 출력 뉴런 개수를 1로 변경

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.sigmoid(self.fc3(x))  # 이진 분류를 위해 시그모이드 함수 사용
        return x

# 데이터셋 변환 및 로드
transform = transforms.Compose([
    transforms.Grayscale(),  # 흑백 이미지로 변환
    transforms.Resize((28, 28)),  # 이미지 크기를 28x28로 조정
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

train_dataset = CustomDataset(data_config['train'], transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# 손실 함수 정의
criterion = nn.BCELoss()  # 이진 교차 엔트로피 손실 함수 사용

# 모델 초기화
model = SimpleNN()

# 최적화 알고리즘 선택
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 모델 훈련
num_epochs = 20
for epoch in range(num_epochs):
    running_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        outputs = outputs.squeeze(dim=1)  # 2D 텐서를 1D 텐서로 변환
        loss = criterion(outputs, labels.float())  # BCELoss를 사용하므로 레이블을 float으로 변환
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')

# TorchScript로 모델 저장
example_input = torch.rand(1, 1, 28, 28)  # 예제 입력 생성
traced_model = torch.jit.trace(model, example_input)
traced_model.save("constants.pt")

--Intel® oneAPI Math Kernel Library: Windows*
https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl-download.html?operatingsystem=windows&windows-install=offline

--cpu로 하는데 계속 에러남 
INTEL MKL ERROR: 지정된 모듈을 찾을 수 없습니다. mkl_avx2.1.dll.
Intel MKL FATAL ERROR: Cannot load mkl_avx2.1.dll or mkl_def.1.dll. 

mkl_avx2.1.dll
mkl_def.1.dll 이파일들을 계속 설정해줘도 에러나 남


--gpu로 하기로함
--posershell에서
# 파일 다운로드
Invoke-WebRequest -Uri "https://download.pytorch.org/libtorch/cu111/libtorch-cxx11-abi-shared-with-deps-1.10.0%2Bcu111.zip" -OutFile "libtorch-cxx11-abi-shared-with-deps-1.10.0+cu111.zip"

# 압축 해제
Expand-Archive -Path "libtorch-cxx11-abi-shared-with-deps-1.10.0+cu111.zip" -DestinationPath "libtorch"

-- 2.3.0+cu121은 CUDA 12.1을 사용하는 GPU 지원 버전
-- 설치되어있는 CUDA Toolkit 버전(11.8)이안맞다고해서 
CUDA Toolkit 12.1 Downloads 설치
-- 계속 cuda로드에러가 나서 pytorch를 cuda를지원하도록 다시빌드하기위해서 pytorch를 다운받는다
git clone --recursive https://github.com/pytorch/pytorch
