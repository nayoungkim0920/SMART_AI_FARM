- 모델 평가 척도(Evaluation Metric)
  - 분류(Classification) 평가 척도
    - 정확도(Accuracy)
    - 정밀도(Precision)
    - 재현도(Recall)
    - F1 score
  - 예측(Regression) 평가 척도

- Confusion matrix(혼동행렬, 오차행렬)
분류 모델이 두 개 이상의 클래스를 구분하는 경우, 모델이 얼마나 잘못된 클래스를 예측하였나
하는 '혼동'이나 '오차'를 행렬로 나타낸 것
시스템이 두 개의 클래스를 얼마나 헷갈려 하는지를 쉽게 알 수 있는 지표
지도 학습으로 훈련된 분류 알고리즘의 prediction(예측)성능을 측정하기 위해 예측값의 실제값을
시각화할 수 있는 표이다.

- 정확도(Accuracy)
전체 중 정답 비율을 계산해주는 방법이다. 정확도의 값이 높을 수록 예측 정확도가 높다고
할 수 있다. 하지만 정확도 측정은 자칫하면 편향의 함정에 빠질 수 있다. 
정확도는 전체 예측에서 정답을 맞힌 건수의 비유로, 정답 Positive든 Negative와 상관없이
맞히기만하면 된다고 보기 때문이다.
예를 들어, 이상징후에 따라 대한 민국 4월에 눈이 내릴지 여부에 대한 예측을 한다고 한다.
이럴 때는 머신러닝들 사용할 필요도없이 무조건 Negative를 예측한다면 99.9% 정도의 정확도가
나올 것이다. 4월에 눈이 내릴일은 거의 없으니까말이다.
따라서 정확도를 측정할 땐 데이터가 imbalanced 할 시엔 적합하지 않은 평가지표로, 
balanced 된 데이터에서 사용하는 것이 적합하다.

- 재현율(Recall)
실제 데이터에 Negative 비율이 너무 높아 희박한 가능성으로 발생할 상황에 대해 제대로 된 분류를
해주는지 평가할 수 있는 지표이다. 따라서 애초에 True가 발생하는 확률이 낮을 때 사용하면 
적합하다.
대한민국에 4월에 눈이 내릴지 여부에 대한 예측의 경우 recall을 계산하여 실제로 눈이 내린
날짜 중 몇 개나 맞히는지 확일할 수 있다. 단순히 accuracy를 계산할 때 False로
예측했다면 99%가 되겠지만, recall 방식대로 계산하면 True Positive는 계산하 수 없으니 recall이
0이 된다.
Recall과 Precision은 서로 반대되는 개념의 지표로 서로 트레이드 오프의 관계이다.
이 두 가지 중 어느 거승ㄹ 우선 순위에 둘 지를 염두해주며 한족의 쉬를 낮추거나 두 수치의 균현점을
찾아주어야 한다. 높은 수준의 정밀도를 요구할 시 반드시 최소 재현율의 기준을 물어보아 맞추어야 한다.
Recall과 Precision 두 방법 모두 유용한 지표이지만 두가지를 모두 고려한 분류 평가 수치도 존재한다.
이 수치를 F1 Score라고 한다.

- F1 Score
Precision과 Recall의 harmonic mean으로 두 가지의 조화 평균을 구하는 방법이다.
두가지 지표를 모두 균형 있게 반영하여 모델의 성능을 확인하기 위함이다.
Accuracy는 균형(Balanced)된 데이터에 적합한 매트릭. 
F1 Score는 보통 불균형(Imbalanced) 분류 문제에서 평가 척도로 주로 사용된다.
데이터가 불균형한 상태에서 Accuracy로 성능을 평가하기엔 데이터 편향성이 너무 크게 나타나 올바르게
성능을 측저하기 힘들게 된다.
불균형 상태에서 조화 평균 함으로써 큰 값의 가중치를 낮추고 작은 값에 더 맞추는 과정을 거치며 값의
크기가 상쇄되기 때문에 F1 Score를 평가 척도로 사용한다.

