- 모델 평가 척도(Evaluation Metric)
  - 분류(Classification) 평가 척도
    - 정확도(Accuracy)
    - 정밀도(Precision)
    - 재현도(Recall)
    - F1 score
  - 예측(Regression) 평가 척도

- Confusion matrix(혼동행렬, 오차행렬)
분류 모델이 두 개 이상의 클래스를 구분하는 경우, 모델이 얼마나 잘못된 클래스를 예측하였나
하는 '혼동'이나 '오차'를 행렬로 나타낸 것
시스템이 두 개의 클래스를 얼마나 헷갈려 하는지를 쉽게 알 수 있는 지표
지도 학습으로 훈련된 분류 알고리즘의 prediction(예측)성능을 측정하기 위해 예측값의 실제값을
시각화할 수 있는 표이다.

- 정확도(Accuracy)
전체 중 정답 비율을 계산해주는 방법이다. 정확도의 값이 높을 수록 예측 정확도가 높다고
할 수 있다. 하지만 정확도 측정은 자칫하면 편향의 함정에 빠질 수 있다. 
정확도는 전체 예측에서 정답을 맞힌 건수의 비유로, 정답 Positive든 Negative와 상관없이
맞히기만하면 된다고 보기 때문이다.
예를 들어, 이상징후에 따라 대한 민국 4월에 눈이 내릴지 여부에 대한 예측을 한다고 한다.
이럴 때는 머신러닝들 사용할 필요도없이 무조건 Negative를 예측한다면 99.9% 정도의 정확도가
나올 것이다. 4월에 눈이 내릴일은 거의 없으니까말이다.
따라서 정확도를 측정할 땐 데이터가 imbalanced 할 시엔 적합하지 않은 평가지표로, 
balanced 된 데이터에서 사용하는 것이 적합하다.

- 재현율(Recall)
실제 데이터에 Negative 비율이 너무 높아 희박한 가능성으로 발생할 상황에 대해 제대로 된 분류를
해주는지 평가할 수 있는 지표이다. 따라서 애초에 True가 발생하는 확률이 낮을 때 사용하면 
적합하다.
대한민국에 4월에 눈이 내릴지 여부에 대한 예측의 경우 recall을 계산하여 실제로 눈이 내린
날짜 중 몇 개나 맞히는지 확일할 수 있다. 단순히 accuracy를 계산할 때 False로
예측했다면 99%가 되겠지만, recall 방식대로 계산하면 True Positive는 계산하 수 없으니 recall이
0이 된다.
Recall과 Precision은 서로 반대되는 개념의 지표로 서로 트레이드 오프의 관계이다.
이 두 가지 중 어느 거승ㄹ 우선 순위에 둘 지를 염두해주며 한족의 쉬를 낮추거나 두 수치의 균현점을
찾아주어야 한다. 높은 수준의 정밀도를 요구할 시 반드시 최소 재현율의 기준을 물어보아 맞추어야 한다.
Recall과 Precision 두 방법 모두 유용한 지표이지만 두가지를 모두 고려한 분류 평가 수치도 존재한다.
이 수치를 F1 Score라고 한다.

- F1 Score
Precision과 Recall의 harmonic mean으로 두 가지의 조화 평균을 구하는 방법이다.
두가지 지표를 모두 균형 있게 반영하여 모델의 성능을 확인하기 위함이다.
Accuracy는 균형(Balanced)된 데이터에 적합한 매트릭. 
F1 Score는 보통 불균형(Imbalanced) 분류 문제에서 평가 척도로 주로 사용된다.
데이터가 불균형한 상태에서 Accuracy로 성능을 평가하기엔 데이터 편향성이 너무 크게 나타나 올바르게
성능을 측저하기 힘들게 된다.
불균형 상태에서 조화 평균 함으로써 큰 값의 가중치를 낮추고 작은 값에 더 맞추는 과정을 거치며 값의
크기가 상쇄되기 때문에 F1 Score를 평가 척도로 사용한다.

- 모델 성능평가
모델의 성능(정확도)을 Mean average precision(mAP)를 통해 확인한다.
mAP가 높을 수록 정확하고, 작을수록 부정확하다.
Average precision(AP)계산을 할 때 precision-recall 곡선을 사용하는데,
관련 내용들을 이해하기 위해서IoU, precision, recall, precision-recall의 개념을
먼저 살펴봐야 한다.

1)Intersection over union(IoU)
IoU는 어떤 데이터셋에 대하여 객체 검출하는 모델의 정확도를 측정하는평가지표이다.
합성곱 신경망을 사용한 객체 검출 모델 Convolutional Neural Network detectors(
R-CNN, Faster R-CNN, YOLO, ...)을 평가할 때 사용된다.
IoU로 평가하기 위해서는 ground-truth bounding box(정답 바운딩 박스)와 모델로부터
예측된 predicted bouding box가 필요하다.
사람이 사물의 위치에 라벨링한 정답 바운딩 박스 초록색 ground-truth bounding box,
모델이 예측한 바운딩 박스 빨강색 predicted bouding box
IoU(객체 검출하는 모델의 정확도를 측정) 계산방법 = 교집합 영역 넓이 / 합집합 영역 넓이
"Correct" if IoU >= 0.5

Classification(분류) 문제같은 경우에는 모델이 예측한 클래스라벨 값이 실제 클래스 라벨 값과
같냐 다르냐 만따지면 된다. 
하지만 객체 검출에서는 모델이 예측한 바운딩 박스의 x,y 좌표들이 정답 바운딩 박스의 x,y좌표들과
정확히 일치하는 경우는 거의 없다. 그래서 IoU를 사용하는 것이다.
IoU는 모델이 예측한 바운딩 박스가 정답 바운딩 박스와 겹치는 부분이 많으면 많을 수록
rewards를 준다. 이렇게 IoU를 사용함으로써 x,y좌표가 정확히 일치하느냐를 보는게아니라
예측한 바운딩 박스가 정답과 최대한 가까워 지도록 학습하는 것이다.
두 box의 크기가 동일할 경우 두 box의 2/3는 겹쳐줘야 0.5의값이 나오기 때문에 
R-CNN에서는 보통 IoU의 threshold값으로 0.5를 잡아서 해당 region을 객체로 바라보고
ground truth와 같은 class로 labeling을 한다고 한다.
IoU가 mAP를 이해하는데 필요한 이유가 뭘까
바로 다음에 나올 precision과 recall을 계산할 때 물체를 검출했을 때 옳게 검출되었다와
옳게 검출되지 않았다를구분해주는 기준이 필요한데그 기준으로 IoU가 쓰인다.
예시) MS COCO mAP 기준으로 YOLOv3 모델은 33.0 < RetinaNet모델은 40.8
      Pascal VOC mAP(AP50) 기준으로 YOLOv3은 57.0 = RetinaNet 61.1(속도는 YOLOv3가 4배 빠름)
Pascal VOC는 IoU(Intersecction over Union) > 0.5인 detection은 true, 그 이하는 
false로 평가하는 방식을 사용하고,
COCO는 IoU > 0.5, IoU > 0.55, IoU > 0.6, ..., IoU > 0.95 각각 기준으로 AP를
계산한 후 이들의 평균을 취하는 방식을 사용한다.
- mAP@.5 : 0.995 Pascal VOC의 mAP평가 방식
- mAp@.5:.95 : 0.93 COCO의mAP평가 방식

Precision, Recall
precision(정밀도) : 모델이 예측한 결과의 Positive 결과가 얼마나 정확한지를 나타내는 값
Recall(재현율) : 모델이 예측한 결과가 얼마나 Positive 값들을 잘 찾는지 측정
Precision(정밀도) = TP / TP + FP
Recall(재현율) = TP / TP + FN
TP : P -> P
FP : N -> P
FN : P -> N
TN : N -> N
Precision(정밀도) : 검출 결과들 중 옳게 검출한 비율을 의미한다.
알고리즘이 사람 10명을 검출했는데 그중 4명을 옳게 검출해낸 것이라면
Precision은 4/10 = 0.4이다.
Precision = TRUE detections / whole detections of an algorithm
Recall(재현율) : 실제 옳게 검출된 결과물 중에서 옳다고 예측한 것의 비율을 의미
10개의 옳게 검출된 결과물 중에서 옳다고 예측한것이 5개라고 예측했다면 
recall은 5/10 = 0.5가 된다.
Recall = detected TRUE / total number of existing TRUE

Precision만으로 물체 검출 알고리즘의 성능을 평가하는 것은 적절하지 않다.
또한 Recall만으로 성능을 평가하는 것도 적절하지 않다.
Precision과 Recall은 항상 0과 1사이의 값으로 나오게 되는데 
Precision(정밀도)이 높으면 Recall(재현율)은 낮은 경향이 있고
Precision이 낮으면 Recall이 높은 경향이 있다. 따라서 두값을 종합해서 알고리즘의 성능을
평가해야하고 그래서 나온 것이 Precision-recall 곡선 및 AP다.
물체를 검출했을 때 게 검출되었다와 옳게 검출되지 않았다를 구분해주는 기준,
즉 TP와 FP를 결정해주는 기준이 바로 intersection over union(IoU)이다.

Precision-recall 곡선(PR곡선)
PR곡선은 confidence threshold값에 따른 precision과 recall의 값의 변화를 그래프로
표현한 것이다.
confidence는 알고리즘이 검출한 것(detect한 것)에 대해 얼마나 정확하다고 생각하는지
알려주는 값인데, 만약 어떤 물체를 검출했을 때 confidence레벨이 0.99라면 알고리즘은 
그 물체가 검출해야하는 물체와 거의 똑같다고 생각하는 것과 같다.
검출된 모든 바운딩 박스들을 confidence 내림차순으로 정렬을 한다. 그리고 각각 
precision과 recall값을 계산한다.

Average Precision(AP)
precision-reall 그래프는 어떤 알고리즘의 성능을 전반적으로 파악하기에는 좋으나 서로 
다른 두 알고리즘의 성능을 정량적으로 (qunatitatively)비교하기에는 불편한 점이 있다.
Average precision은 인식 알고리즘의 성능르 하나의 값으로 표현한 것으로 precision-recall
그래프에서 그래프 선 아래쪽의 면적으로 계산된다.
average precision이 높으면 높을 수록 그 알고리즘의 성능이 전체적으로 우수하다는 의미
이고, 컴퓨터 비전 분야에서 물체 인식 알고리즘의 성능은 대부분 average precision으로
평가한다.

Mean Average Precision(mAP)
1. 모델이 예측한 모든 바운딩 박스를 가져올 것이다.
빨간 바운딩 박스는 에측 값들이고 초록색 바운딩 박스는 타겟값(정답값)들이다. 
빨간 바운딩 박스를 보면 박스마다 점수가 박힌다. 이점수는 바운딩 박스가 자신이 
예측한 바운딩 자신이 예측한 바운딩 박스 안에 객체가 있다고 믿는 확률값confidence score이다.
yolov5에서 conf_thres를 인자값을 따로 지정해주지 않으면 default 0.00로 설정되어있다.
모든 바운딩 박스를 가지고 와야함으로 원래는 0으로 해야하지만,
너무 느리기 때문에 0.00로 설정했다고 한다. 
이 conf_thres값은 mAP계산만을 위한 값이다. 그러므로 inference code(detect.py)를
돌릴 때는 conf_thres값이 기본 0.25로 설정되어 있다(mAP 계산 용이 아닌 예측 용).
이 경우에는 더 높은 값으로 변경해도 상관없다(0 ~ 1 사이).

confidence score가 각각 0.3, 0.6, 0.7인 3개의 바운딩 박스가 있다.
이 바운딩 박스들이 True Positive(TP) / False Positive(FP)인지 평가한다고 한다면,
cofidence score가 0.3인 첫번째 바운딩 박스는 어떤 타겟 바운딩 박스와도 겹치는 부분이
없다. 그러므로 첫번째바운딩 박스는 FP이다.
confidence score가 0.6 인 두번째 바운딩 박스는 강아지의 한 부분을 검출하긴 했지만 
0.5보다 작은 IoU를 가지고 있음으로 FP이다.
confidence score가 0.7인 세번째 바운딩 박스를 보면 타겟 바운딩 박스와 겹치는 부분이
많고 0.5보다 큰 IoU를 가지고 있음으로 TF이다.

하나의 예측된 바운딩 박스를 가지고 있고 TP이다.

세계의 예측된 바운딩 박스를 가지고 있고, 이미지의 위쪽에 위치한 2ㄹ개의 바운딩 박스는
FP이고, confidence가 0.9인 바운딩 박스는 0.5보다 큰 높은 IoU를 가지고 있음으로 TP이다.

모든 바운딩 박스를 하나의 표로 정리해보고 바운딩 박스들을 내림차순한다.

Image  Confidence  TP(P->P)/FP(N->P)
3      0.9          TP
3      0.8          FP
1      0.7          TP
1      0.6          FP
2      0.5          TP
1      0.3          FP
3      0.2          FP

https://lynnshin.tistory.com/48
